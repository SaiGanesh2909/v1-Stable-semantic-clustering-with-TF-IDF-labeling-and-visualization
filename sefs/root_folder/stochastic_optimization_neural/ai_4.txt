Transformer attention mechanism 4 improves stochastic gradient descent optimization in neural network backpropagation training. 
