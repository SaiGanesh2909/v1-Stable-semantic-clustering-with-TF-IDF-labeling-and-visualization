Transformer attention mechanism 1 improves stochastic gradient descent optimization in neural network backpropagation training. 
